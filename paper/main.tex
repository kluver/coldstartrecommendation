%TODO: update language retain to simulated profile size. do a search ``reta''

\documentclass[letterpaper]{sig-alternate}

\usepackage{hyperref}

\pdfpagewidth = 8.5in
\pdfpageheight = 11in

\usepackage[factor=250,spacing=true]{microtype}

\begin{document}
\conferenceinfo{RecSys '14}{October 6--10, 2014, Foster City, Silicon Valley, USA}
%\CopyrightYear{2007} % Allows default copyright year (20XX) to be over-ridden - IF NEED BE.
%\crdata{0-12345-67-8/90/01}  % Allows default copyright data (0-89791-88-6/97/05) to be over-ridden - IF NEED BE.

\title{Evaluating Recommender Behavior For New Users}

\numberofauthors{2}

\author {
\alignauthor
Daniel Kluver\\
\affaddr{GroupLens Research}\\
\affaddr{Department of Computer Science and Engineering}\\
\affaddr{University of Minnesota}\\
\affaddr{Minneapolis, MN 55455 USA}\\
\email{kluver@cs.umn.edu}
\alignauthor
Joseph A. Konstan\\
\affaddr{GroupLens Research}\\
\affaddr{Department of Computer Science and Engineering}\\
\affaddr{University of Minnesota}\\
\affaddr{Minneapolis, MN 55455 USA}\\
\email{konstan@cs.umn.edu}
}

\maketitle
\begin{abstract}

  The new user experience is one of the important problems in recommender systems.
  Past work on recommending for new users has focused on the process of gathering information from the user.
  Our work focuses on how different algorithms behave for new users.
  We describe a methodology that we use to compare representative of three common families of algorithms along eleven different metrics.
  We find for first few ratings a baseline algorithm performs better than three common collaborative filtering algorithms.
  Once we have a few ratings, we find that Funk's SVD algorithm has the best overall performance.
  Our results can inform the design of interfaces and algorithms for new users.

\end{abstract}

%TODO: update these things.
% A category with the (minimum) three required fields
\category{H.3.3}{nformation Search and Retrieval}{Information filtering}

%TODO: update these things.
\terms{Algorithms, Measurement}

%TODO: update these things.
\keywords{Recommender Systems, Evaluation, Profile Size, New User Experience, New User Problem, User Cold Start}

\section{Introduction}

  % new user experiance is only chance to make all important first impression
  One of the important problems in recommender systems is the new user experience.
  If the system cannot provide a good user experience, then new users are likely to leave and not return.
  Even if the system can make accurate recommendations accuracy alone is not enough to make a good first impression.
  If the system only recommends items the user has seen, then the user may see no value in the system and leave.
  Alternatively, if the system only recommends obscure items, then the user will have no way to know if the recommendations are worth taking.
  This problem is made more complex by how little information we have about new users.
  
% one important trait is trust.
    % trust is complex
    % matching user's experiance is probably important.
  %One important part of how the user interacts with a recommender system is how much the user trusts the recommender system.
  %The more the user trusts a recommender system, the more the user can take advantage of it.
  %If the user doesn't trust a recommender system, the system can be of no value to the user.
  %Building trust with the user is therefore an important part of the new user experience.
  %Past work has shown that one of the strongest factors in the formation of trust is how much the system's recommendations match the user's knowledge \cite{wangAttributionOfTrust}.
  %For new users this can easily take the form of recommending items that the user has already seen.
  %TODO: I kinda of want to cut this. I want to talk about this theme, but I don't actually know where it fits in.

  % past work tends to focus on the algorithm side, the algorithm side of this problem is cold start.
    % SMAL DATA TENDS TO IMPLY BAD RESULTS.
    % cold start algorithms is a larger problem
    % cold start algorithms tend to use extra information to help imrpove eary performance.
    % we only look at user cold start as it is ubiquitous.
  Past work on new user experience in recommender systems has tended to focus on the `cold start problem'.
  Cold start refers to the problem that no algorithm can make highly personal recommendations with very limited amounts of information.
  Unfortunately this means that regardless of the algorithm, no recommender system can make high quality recommendations for users who are still joining the system.
  
  % One common new user strategy is to have a special data collection mode.
  % collect ratings fast to get over a precived ``bad period''
  % work on this focuses on how to collect useful ratings.
  % (framework from decision tree work)
 
  The natural way to improve the system for new users is to have access to some extra information.
  The best studied strategy is for new users to go through a new user rating survey.
  Systems that take this practice do not allow their users to receive recommendations until some fixed number of ratings have been entered.
  The idea is that after enough ratings have been entered, the predictions will be `good enough'.
  This idea has been explored by many authors, most consider a system in which the user is presented with a list of items, and asked to rate items until they have enough ratings.
  For a good discussion of prior work in this direction see \cite{adaptiveBootstrapping}.

  % One promising approach is to let user control interaction
  % user or system driven control.
  % promising, we don't know much about follow up.
  %One promising approach to the new user rating survey is to have users control which movies they rate.
  %This was pioneered by McNee et al., who looked at comparisons between user driven and system driven new user surveys \cite{mcneeInterfaces}.
  %They found that users who were in control of the items they rated made fewer ratings, but that the profiles they created were more useful in predicting the user's future ratings.
  %Interestingly, while these users took more time to create their profiles, they were just as likely as other users to report that the sign up was short.
  %While these results are quite promising, to our knowledge no follow up work has been done to explore these issues further.
  %This goes to show, however, that the new user experience is a complicated issue and novel strategies may yet prove to be the best solution.

  % bridge from past work to our work.
    % point out holes (evaluating standard algorithms for little to no ratings, wholeistic evaluation)
    % point out that other approaches than smart object ordering exist, and we need to be able to compare.

  While past work has covered the new user rating survey from many different angles, it has not carefully evaluated which algorithm is best at recommending to new users.
  The new user survey work sidesteps this issue by assuming that the user will receive no personalization until the survey is complete.
  Even for systems with a new user survey carefully choosing algorithms for new users may be very important.
  Users fresh out of the new user survey will still have far fewer ratings than the average user of the recommender system.
  Different algorithms may provide very different experiences for these users, even if they give very similar results for the average user.
  
  Fortunately, there has been a growing trend in recommender system research to perform multiple metric evaluations of recommender systems.
  This work comes out of a pushback against early recommendation evaluations which only evaluated recommenders in terms of their predictive accuracy \cite{accurateIsNotEnough}.
  The response of this pushback has been the generating of many different metrics which allow us to more carefully understand the separate aspects of a recommender's output \cite{LathiaTemporal, CremonesiTopN, zieglerDiversity}.
  %TODO: I want to cite more papers here, I don't think I have good ones either. Joe would probably know.
  This strategy allows us to dig much deeper into the differences between algorithms, looking past simple measures of accuracy to more meaningful dimensions such as how novel the recommendations are, or how frequently the recommendations change for any user.
  This allows us to capture the qualitative tradeoffs between algorithms which are otherwise quite similar, which helps us choose algorithms that are better suited for a particular domain.

  We combine these two types of research, new user experience research, and multiple metric algorithm evaluation.
  Our goal is to understand the differences between algorithms for new users, especially how these algorithms respond to different amounts of information.
  by using multiple metrics we can understand not just what algorithm is best in terms of absolute performance, but also gain a more nuanced understanding of the qualities of different algorithm's recommendations.
  %Furthermore, we do not seek to find the algorithm that scores best on some metric of choice, instead, we seek to form a complete understanding of the differences between algorithms for cold start use.

  We structure our work around the following research question.
  \begin{itemize}
    \item RQ1: How do algorithms behave for users with few ratings?
  \end{itemize}
  Algorithm behavior is a very broad subject.
  We will look at three distinct sub questions to help capture algorithm behavior.
  %We will look at three distinct ways to quantify the behavior of a recommender system. each of which will lead to multiple metrics.

% RQs
    % RQ1 How do the behavior of different algorithms change with the number of ratings?
      % Explain that there are a lot of different ways to measure algorithm behavior.
        % Predictive accuracy, commonly measured with MAE or RMSE
          % coverage
        % Value of recommendations, tradidionally done with metrics from information retrieval such as precision, recall, MAP, etc. These algorithms try to measure ...
          % cite the John/Joe paper (RMSE is not enough)
          % cite the paper cited by 10 is enough
          % topN RMSE
        % There are many properties of recommenders that are (find words here)
          % popularity
          % diversity
          % spread
      
  \begin{itemize}
  \item RQ1.a: How well can different algorithms predict the future ratings of new users?
  \item RQ1.b: How well can different algorithms rank and recommend good items to new users?
  \item RQ1.c: How do algorithms behave as measured by other metrics such as popularity and diversity for new users?
  \end{itemize}

  We will answer these questions by developing an evaluation framework that can be used to understand how algorithms perform for new users.
  This framework will compare algorithms on eleven different metrics to allow a more complete understanding of the differences between algorithms.
  We will then use this framework to study how three common algorithms behave for new users.
  Finally, we will draw conclusions from our evaluation and outline directions for future work.

\section{Methodology}
\label{sec:methodology}
  % (high level description/restatement of approach) To answer our research questions we will various metrics of recommenders in an offline setting simulations users as they join the system.
    % maybe hit some limitations
    % motiviate _why do a simulation_ (a lot of metrics we want to try)

  To answer our research questions we will evaluate algorithms in an offline simulation designed to simulate users as they join the system.
  We do this by adopting the standard train and test methodology to simulate users with a given number of ratings.
  We run this methodology for various profile sizes, and then plot the results.
  This lets us understand both how the algorithms compare on these metrics, and how the algorithm performance changes with the number of ratings provided.

  This, or a similar technique, has been used by previous authors to make plots of metric values over profile size \cite{DrennerInitialExperiance, TenIsEnough, AdaptiveBootstrap, LathiaTemporal}.
  Past work used these plots to establish that more ratings lead to better results.
  We take this technique farther and develop a much more nuanced understanding of the different algorithms, and how they compare for different profile sizes.

  Our approach is related to the temporal evaluation methodology, in which a recommender is repeatedly trained on all ratings up to a point in time, and then tested using the next ratings to be made for each user.
  Temporal evaluations have been used in the past to evaluate how the list of recommended items change as users enter more ratings into the system \cite{LathiaTemporal}.
  While the output of a temporal evaluation can be very similar to our own, the methodologies have some key differences.
  Where a temporal evaluation is designed specifically to consider the order in which users make ratings, our methodology is designed to try to eliminate this ordering effect from our consideration of the algorithm's behavior.

  As mentioned earlier, many recommender systems use a new user survey to collect the first user ratings.
  These ratings are different than normal user ratings.
  As we explain shortly, our methodology uses randomization in key places to remove this ordering effect.
  This allows us to better capture the underlying algorithm performance on ``normal'' user data.
  While this makes our methodology unsuited for applying temporal metrics, it allows us to separate an algorithm's behavior from how ratings are collected for new users.
  Therefore, we feel that our methodology is more suitable to answer our research question.

  \subsection*{Algorithms}
  % algorithms (discuss tuning of algorithms per algorithm?)
  % motivation for algorithm selection
  % list algorithms 

  We seek to develop an understanding of how a range of different standard algorithms perform at for new users.
  Rather than picking advanced algorithms that are tuned for new user performance, we decided to first understand how representatives of three common types of algorithms perform.
  Therefore we will compare User-User\cite{resnick1994grouplens}, Item-Item\cite{sarwar2001item}, and Simon Funk's SVD\cite{funk_netflix_2006} algorithms against two simple baseline algorithms.
  More information about these algorithms can be found in table \ref{tbl:algo}
  Code for our evaluation, using the Lenskit evaluation framework \cite{lenskit} is available at \url{https://bitbucket.org/kluver/coldstartrecommendation}.
  
  \begin{table}[ht!]
    \centering
    \begin{tabular}{|p{6em}|p{18em}|}
      \hline
      Algorithm          & Algorithm description \\\hline
      ItemBaseline       & Item's average rating, with mild bayesian damping towards the global mean \cite{funk_netflix_2006}. \\\hline
      UserItem\-Baseline & ItemBasline adjusted by the user's average offset from the ItemBaseline. Mild mean damping is applied \cite{funk_netflix_2006}. \\\hline
      User-User          & User based nearest neighbor collaborative filtering \cite{resnick1994grouplens} with a neighborhood size of 30 and ratings normalized by subtracting the UserItemBaseline score. \\\hline
      Item-Item          & Item based nearest neighbor collaborative filtering \cite{sarwar2001item} with a neighborhood size of 30 and ratings normalized by subtracting the ItemBaseline score.   \\\hline
      SVD                & Simon Funk's SVD based collaborative filtering approach \cite{funk_netflix_2006} with 30 features and 150 training iterations per feature. \\\hline
    \end{tabular}
    \caption{Summary of algorithms.}
    \label{tbl:algo}
  \end{table}
  
  % evaluation and code to run these algortihms available at.
  % citations and lenskit info
  Each algorithm was configured and tuned similarly to Ekstrand's 2012 evaluation \cite{ekstrand2012recommenders}.
  Since this evaluation was on a different dataset (MovieLens 10M vs. MovieLens 1M), we performed minor tuning from these parameters.
  Most notable, we found that the previous damping parameter used for the baselines was too large.
  We found that a value of 5 gave much better results for new user recommendation.
  During tuning we found that our key results are consistent among those configurations that perform well.
  %TODO: carefully run this wording by joe.


  \subsection*{Metrics}

  We use a total of eleven metrics.
  In our evaluation we found that some of these metrics are redundant, and future work should be able to use a much smaller list of metrics.
  The metrics are listed in table \ref{tbl:metrics}.
  \begin{table*}[ht!]
    \centering
    \begin{tabular}{|p{7em}|p{20em}|p{4em}|p{18em}|}
      \hline
      \multicolumn{4}{|c|}{{\bf Accuracy and Ranking Metrics}} \\\hline
      RMSE                   & Standard prediction accuracy metric \cite{handbook}. & 0 to 5 & Smaller is better. \\\hline
      nDCG                   & Standard ranking quality evaluation \cite{handbook}. & 0 to 1 & Larger is better. \\\hline
      \hline
      \multicolumn{4}{|c|}{{\bf Monotonic Recommendation Metrics}} \\\hline
      Precision@20           & Count of recommended items in the test set and rated 4.0 or higher\footnotemark[1] \cite{handbook}. & 0 to 1 & Larger is better. \\\hline
      MAP@20                 & Average of precision at each rank one through twenty \cite{manning2008introduction}. & 0 to 1 & Larger is better.\\\hline
      Fallout@20             & Count of recommended items in the test set and rated 2.0 or lower\footnotemark[1] \cite{handbook}. & 0 to 1 & Smaller is better. \\\hline
      MeanRating@20          & Average rating of recommended items that are in the test set.& 0 to 5 & Larger is better \\\hline
      RMSE@20                & RMSE computed only over those items that were recommended.& 0 to 5 & Smaller is better \\\hline
      \hline
      \multicolumn{4}{|c|}{{\bf Nonmonotonic Recommender Metrics}} \\\hline
      SeenItems@20           & Count of recommended items in the test set. & 0 to 20 & Not well understood. \newline Larger is \emph{probably} better within reason. \\\hline
      Average\-Popularity@20 & The average number of users in the training set that have seen recommended items \cite{zieglerDiversity}.& 1 to 6000 & Not well understood. \newline Smaller is \emph{probably} better within reason. \\\hline
      AILS@20                & The average pairwise similarity between recommended items \cite{zieglerDiversity}. & 0 to 1 & Not well understood. \newline  Smaller is \emph{probably} better within reason. \\\hline
      Spread@20              & The Shannon's entropy of the distribution of recommended items for users in the test set. & 0 to 12 & Not well understood. \newline Larger is \emph{probably} better within reason. \\\hline
    \end{tabular}
    \caption{Summary of metrics}
    \label{tbl:metrics}
  \end{table*}
  \footnotetext[1]{We also tried 5.0 and 1.0 as cutoffs respectively, and found no meaningful difference.}\addtocounter{footnote}{1}

  We split our metrics into three groups roughly along our three sub research questions.
  
  \paragraph{Accuracy and Ranking Metrics}
  The first two metrics evaluate the recommender based on its predictions for each item.
  These evaluate, the accuracy of the predictions, and the ability of the recommender to rank the list of all items.
  We find that these two metrics perform quite similarly in our evaluation.

  \paragraph{Monotonic Recommendation Metrics}
  These metrics all evaluate the recommendations produced by the recommender.
  For recommendations we will use the 20 highest predicted items for each user, excluding items in the training set.
  As the UserItem-Baseline and ItemBaseline make the same recommendations for all users, we will only report results for the ItemBaseline here.
  The choice of 20 items for the evaluations was arbitrary, testing with other values showed similar results, except on one metric.
  This issue will be addressed later in the results section.
  
  There metrics are useful because these metrics have a well defined monotonic relation to the satisfaction of the user.
  As the metric grows larger, or smaller (depending on metric), we expect the output recommendations to satisfy users more.
  This makes these metrics easier to understand than our later metrics.

  Our first three metrics (Precision, MAP, and Fallout) are standard information retrieval metrics.
  As we show later, we found Precision, MAP, and fallout to not be useful for this evaluation.
  Therefore, to help understand the quality of our recommendations, we also include MeanRating@20, and RMSE@20.

  MeanRatings@20, and RMSE@20 both try to establish how much the user likes the recommendations.
  MeanRatings@20 tells us directly how much the user tends to like recommended movies.
  RMSE@20 accompanies this and lets us know if the recommended items are likely to be mispredicted, which can explain why users may or may not like their recommendations.
  As it helps us interpret these two metrics we will also discuss SeenItems@20 with these metrics, although SeenItems@20 does not have as simple a relation to user satisfaction.

  \paragraph{Nonmonotonic Recommendation Metrics}
  Our final four metrics are all nonmonotonic measures of the user's recommendations.
  These metrics are harder to understand than the monotonic measures of recommendation quality.
  Unlike the previous metrics, we expect that both of the extremes of this metric would be perceived negatively by the user.

  We use these metrics as they can help us understand important properties of how users may perceive the recommendations.
  For example, as mentioned in the introduction how many of the recommended items the user has seen (estimated by SeenItems@20) can have a major impact on how much users trust the recommendations.
  If an algorithm recommends too few items that the user has seen then we expect the user will have trouble evaluating if the recommendations are worth taking.
  That said, however, if too many items in the recommendations have already been seen, the user may feel that the recommendations are not interesting or useful.
  Unfortunately, without more research on these metrics to establish a mapping between user satisfaction and these values we will have to make educated guesses at what values for these metrics are too extreme.

  The average popularity metric is a crude metric of how novel the recommendations are to the user \cite{zieglerDiversity}.
  Broadly speaking we expect users will prefer recommendation lists containing more novel (less popular) items.
  However, if the recommended items are too unpopular we risk that the user will not have heard of the items, which could make the user doubt the recommendations, hurting the user experience.

  Past work has shown that users like their recommendations to be more diverse \cite{zieglerDiversity, martijnDiversity}.
  The belief is that by providing more diverse lists we are giving the user a larger range of items to choose from.
  This can make choosing an item easier as the user is less likely to have to compare very similar options.
  It has also been shown, however, that too much diversity can lead to less user satisfaction \cite{zieglerDiversity}.
  This is probably due to a tradeoff between how diverse a list is and how well it captures a user's tastes.
  At some point, a list cannot become more diverse without include items that the user is not interested in.
  We will measure diversity with the AILS@20 metric.

  The AILS@20 metric is based on the ILS metric introduced by ziegler et al \cite{zieglerDiversity}.
  AILS@N is simply a rescaling of ILS to be scale free with regards to N, this was done to make the metric easier to interpret, by putting it on the same scale as similarity values.
  We used the same item similarity metric for AILS@20 as we used in the Item-Item recommender.

  The Spread@20 metric is designed to measure how well the recommender spreads its attention across many items.
  We expect that algorithms with a good understanding of its users will be able to recommend different items to different users.
  Like diversity, however, we expect that it is not possible to reach complete spread (recommending each item an equal number of times) without making avoidably bad recommendations.
  When Spread is large we know that the recommender is recommending many items with a relatively even distribution, this might be expected from a random recommender.
  When Spread is small, the recommender is focusing on a small set of items which it recommends to every user, this might be expected from a baseline recommender.

  Spread is computed by computing recommendations for each test user.
  Let $C(I)$ be the count of how many times item $I$ showed up in the recommendations.
  Let $P(I) = C(I) / \sum_I C(I)$ be the probability that a random recommendation is item $I$.
  Finally we define spread as the Shannon's entropy of $P(I)$, $Spread = -\sum_I P(I) \log(P(I))$.

  \paragraph{Expectations about algorithm performance}
  % provide expectations.
  We expect that the item baseline algorithm will have approximately constant performance on all metrics.
  Since the Item Baseline is such a simple algorithm we expect that any reasonable algorithm should be able to do better on it the monotonic metrics.
  Furthermore, we expect the improvement over the item baseline to be gradual, with larger improvements only happening with more data.

  The user-item baseline should only differ from the item baseline for metrics that use the prediction value.
  For all other metrics these two should provide the same responses.
  As the user-item baseline is still quite a simple baseline, we expect that the non baseline algorithm will beat this baseline as well.
  That said, for small rating values its possible that the improvements over this baseline will be very small.
  
  We expect that the baseline algorithms will provide the most popular recommendations, with the least diversity, and the least spread.
  We expect that any personalized algorithm should be able to make more nuanced recommendations than simply recommending the best rated items, which should translate as gains on these metrics.
  Furthermore, we expect that as the algorithm learns more about the user it should decrease in popularity, and increase in diversity, and spread, as it becomes able to make more personal and nuanced recommendations to the user.


  \subsection*{Dataset}
  % dataset
    % # user
    % # items
    % distributions of # ratings per user (important: min 20)
    % establish range of evaluation.

  For this evaluation we will use the MovieLens 1M dataset \footnote{\url{http://grouplens.org/datasets/movielens/}}.
  This dataset contains 1 million ratings from 6000 users on 4000 movies.
  
  These ratings were collected from the MovieLens system, which requires 15 ratings in a new user survey before users can enter the system.
  To the best of our knowledge, at the time this data was collected the new user survey preferred items that were very popular.
  Therefore, as we cover shortly, will randomize the order of the ratings to try to avoid measuring the effect of these initial ratings, and instead focus on the algorithm's performance on normal ratings.
  
  Each user in the dataset has at least 20 ratings.
  Therefore we will evaluate new users with up to 19 ratings.
  This will allow us to ensure that each user in the dataset has at least one test rating.

  
  \subsection*{Evaluation Procedure}

  % subsampling strategy
  % describe normal crossfold solution
  The natural way to perform this evaluation is with a modified user crossfolding strategy.
  The normal 5 fold cross validation strategy splits the users into five groups.
  In each of five folds all ratings from four of the groups will be kept in the training set, and then either some constant number or percent of ratings from each user in the fifth group will be set aside for the test set.
  The selection of a test set can be random to avoid any possible ordering effects in the data due to a new user rating strategy.
  To understand how metrics change with a different number of users we can simply generate separate crossfolds for each number of ratings we wish to retain.
  At each crossfold, rather than only taking a constant number of ratings for the test set, we can instead only keep a constant number of ratings for the training set.
  
  % describe issues with solution
  Unfortunately, we found that this strategy can causes biases in our evaluation.
  In particular, this strategy led to the size of the test sets to change as a function of the number of ratings retained.
  As we kept more ratings for the training set, we would have fewer ratings in the testing set.
  This turned out to bais our results, as some metrics, such as nDCG, proved to be sensitive to the number of items in the testing set.
  
  % describe our crossfold then subsample solution.
  Our solution to this bais is to subsample an existing crossfold.
  Therefore, to generate training and test sets for different number of ratings from 1 to 19, we first generate a set of test and training splits with 19 training items using the crossfold strategy outlined above.
  Then, to generate any other number of ratings, we randomly downsample the 19 item training set.
  This allows us to generate training sets with a given number of ratings retained for test users, such that each number of ratings retained uses the same tests set.


\section{Results}

  Using the methodology described in section \ref{sec:methodology} we generate plots for each metric showing how the five algorithms perform new users, versus how many of the user's ratings were retained in the training set.

  
\subsection{Accuracy and Ranking Metrics}

  % Prediction accuracy
\begin{figure*}[ht!]
  \centering
  \includegraphics[width=0.75\linewidth]{../lenskit/output/ekstrandTuned20/accuracy.pdf}
  \caption{Accuracy and Ranking Algorithms}
  \label{fig:rmse}
  \label{fig:ndcg}
\end{figure*}
    % RMSE plot
    % point out error bars etc.
    % explain / interpret plot
  \paragraph{RMSE}
  Figure \ref{fig:rmse} shows how accurate our algorithms are as measured by RMSE.
  User-User, and SVD both behaved essentially as expected.
  SVD shows no improvement with only one rating, and User-User, actually appears to get worse with the first rating, but other than that, these algorithms outperform the item baseline.
  Interestingly, the UserItem-Baseline outperforms both algorithms until around 8 ratings, at which point both algorithms start to perform better.
  Given the size of the differences between UserItem-Baseline, UserUser, and SVD we conclude that all three provide about the same accuracy for new users.
  That such a simple algorithm can outperform more complicated for such small values goes to show that recommending for users with very few ratings is a hard problem.
  
  Item-Item performs quite poorly for new users.
  Until we have 13 ratings, Item-Item is less accurate than the ItemBaseline.
  In fact, for the first two ratings Item-Item appears to perform \emph{worse} as more ratings are added.
  This trend is a result of the limited coverage of the Item-Item algorithm.
  If you consider the accuracy of RMSE on only those items where it makes a personalized prediction it does show a monotonic increase in accuracy similar to the other algorithms.
  However, for small rating counts ItemItem can only predict for some percent of the item (around 60\% for users with two ratings, and around 75\% for users with four ratings).
  Because of this, many of the predictions come from the better performing UserItem-Baseline.
  As we get our first few ratings, the predictions become less likely to be from the UserItem-Baseline, leading to an artificial upward trend in RMSE.
  Fortunately, Item-Item seems to improve accuracy faster than the other algorithms, and therefore appears as if it will catch up.
  Unfortunately, this result suggests that ItemItem may be unsuitable for new users.


  % Recommendation quality metrics
    % NDCG
  \paragraph{nDCG}
  Figure \ref{fig:ndcg} shows the algorithm's performance on the nDCG metric.
  The results for nDCG are very similar to the RMSE results.
  Both UserUser and SVD take an initial hit to accuracy at the first rating.
  This time UserUser takes much longer to recover from this and only beats the baseline after around 12 ratings.
  Again ItemItem performs quite poorly.
  This leaves SVD to perform the best, passing the baseline after about 3 ratings.
  Again we find that our best algorithm for the first few ratings is our baseline.
  
    % MAP, precision, fallout plots 
    % explain results from MAP and precision
    % explain recall results
    % recall results, strangeness and popularity


\subsection {Monotonic Recommendation Quality Metrics}

\begin{figure*}[ht!]
  \centering
  \includegraphics[width=\linewidth]{../lenskit/output/ekstrandTuned20/TopNPrecision.pdf}
  \caption{Information Retrieval Recommendation Quality Metrics}
  \label{fig:map}
\end{figure*}
\paragraph{MAP@20, Precision@20, Fallout@20}
  Figure \ref{fig:map} shows MAP@20, precision@20, and fallout@20.
  All three metrics show essentially the same trend, User-User gets the lowest score, the baseline get the highest score.
  On both MAP@20 and precision@20 svd gets a higher score than Item-Item throughout, performing almost as highly as the baseline.
  On fallout@20 Item-Item and SVD perform about as well as each other, with SVD showing a very slight lead.
  We also tested recall and mean reciprocal rank metrics, but found them to exhibit the same trend, so they were not explored further.

  This result is rather strange as MAP and Precision are metrics where larger scores imply better performance, whereas fallout is a metric where larger scores imply poor performance.
  Therefore we would not expect to see rank equivalent behavior from these metrics.
  Unfortunately this means that these metrics contradict each other in terms of which algorithm performs the best.
  One possible reason for this is that these topN metrics are known to be biased toward recommendations that have more popular items \cite{bellogin}.
  Looking forward to the average popularity (figure \ref{fig:pop}) of recommended items, we see that the average popularity of the recommendations closely replicates the previous three plots.
  Therefore we will discard these metrics, and consider less biased metrics to help us analyze the quality of the recommendations.

  % TopN average rating / topn rmse
    % interpret graph and reason about what it means about quality

\begin{figure*}[ht!]
  \centering
  \includegraphics[width=\linewidth]{../lenskit/output/ekstrandTuned20/rmse_20.pdf}
  \caption{Recommendation Quality Metrics}
  \label{fig:topN.rmse}
\end{figure*}

  \paragraph{SeenItems@20}
  The first graph in figure \ref{fig:topN.rmse} shows the average number of items in the top 20 recommendations for each algorithm that were also in the test set.
  We address this metric out of turn here as it provides important context for the next two metrics.
  The average number of test set items in recommendations shows a similar trend to the MAP, precision, and recall metrics, with user-user getting by far the fewest seen items, and the baseline getting the most.
  As User-User gets less than one seen item on average we found that MeanRating@20 and RMSE@20 metrics were not appropriate for UserUser.
  This was observed in particular when testing our results for sensitivity to recommendation list size.
  We found that different recommendation lists lead UserUser (and only UserUser) to have very different results.
  Because of this issue we will not include User-User in our next evaluations and consider the next two metrics inconclusive for UserUser.
  That said, we consider the small number of seen items itself a bad result for UserUser.
  Put simply, if we cannot reliably estimate if the recommendations are reasonable, how can we expect a user to do so?

  \paragraph{MeanRating@20}
  The second graph in figure \ref{fig:topN.rmse} shows the average rating for those items in the top 20 recommendations for each algorithm.
  The first thing to note about this graph is that all of these algorithms are doing a reasonable job of recommending items the user might like to watch, with all algorithms averaging above 4 stars (implying that the recommended items are, at least on average, enjoyable).
  SVD performs as expected, starting around the baseline for few ratings, and then steadily improving to be the best algorithm after 3 ratings.
  Again, we find that Item-Item performs poorly until quite a few ratings are added, and does worse than baseline throughout.
  This is quite possibly a similar trend to the one we observed with RMSE and nDCG, where ItemItem's performance decreased as coverage increased leading to the curved performance at cold start.
  Regardless, we find that ItemItem's recommendations do not seem to be as well rated as either the baseline or SVD.
    
  \paragraph{RMSE@20}
  The third graph in figure \ref{fig:topN.rmse} shows the RMSE our algorithms got on items in the top 20 recommendations for each algorithm.
  Not surprisingly, this figure shows the same trends as the last one, meaning that the algorithms whose recommendations were (on average) most likable, were those whose recommendations were most accurate.
  Again, SVD performs quite well, easily beating the UserItemBaseline after only 1 rating.
  
  Interestingly, we find that all algorithms are quite a bit more accurate on the top 20 items than they are for the general item collection.
  This is likely an edge effect, where the highest rated (5 star) items are perhaps easier to predict than other items.


\subsection{Nonmonotonic Recommender Quality Metrics}
  % Other recommender metrics

\begin{figure*}[ht!]
  \centering
  \includegraphics[width=\linewidth]{../lenskit/output/ekstrandTuned20/popdiv.pdf}
  \caption{Nonmonotonic Recommendation Quality Metrics}
  \label{fig:pop}
\end{figure*}

  \paragraph{AveragePopularity@20}
    % popularity graph
  The first graph in figure \ref{fig:pop} shows the average popularity of recommended items by each algorithm.
  As expected the baseline algorithms makes the most popular recommendations, followed by SVD, ItemItem, and then UserUser in a distant last.
  As stated earlier, We expect that users will feel the baseline's popularity (around 1000) is too high.
  We also expect that users will perceive User-User's popularity (around 50) as too low.
  SVD and Item-Item both seem to perform well, with SVD generating more popular recommendations than Item-Item.
  SVD and ItemItem both show a trend of decreasing popularity as more ratings are retained, suggesting that these algorithms will make more nuanced, less popular recommendations for users they know more about.
  Interestingly, these algorithms each appear to be converging to their own separate level of popularity, which suggests that this ordering may be independent of the number of ratings.

    % diversity plot
  \paragraph{AILS@20}
  The second graph in \ref{fig:pop} shows a similar trend to the popularity trend.
  Again, we expect that the baseline, which provides the least diverse recommendations, would be precised as not very diverse by users.
  Quite possibly due to the preference for obscure movies, UserUser seems to produce the most diverse lists of any algorithm.
  We expect that the UserUser recommendation lists might be perceived as too diverse.
  Unlike popularity, SVD and ItemItem seem to perform equivalently in terms of diversity.
  We also see that all three collaborative filtering algorithms show a trend of increasing diversity (decreasing  inter-list similarity) as more ratings are retained.

  % spread plot
  \paragraph{Spread@20}
  The third graph in \ref{fig:spread} shows the spread of the recommendations from each algorithm.
  As expected the baseline performs worst on spread, as it recommends the same items to each user (unless the user has already rated one of these items).
  SVD performs mildly better than the baseline, and shows definite improvement as more ratings are added.
  UserUser and ItemItem start at around the same value as each other, however, after only two ratings they have crossed, as ItemItem quickly increases to have the highest spread of all the algorithms, and UserUser quickly decreases into second place.
  Interestingly, only UserUser shows a trend to have less spread ratings as it learns more about the user, ItemItem and SVD both show the expected trend of increasing Spread as they learn more.
  This could be some form of a regression towards the mean, where as it learns more about users its recommendations become less personalized to the users individual quirks.
  More research will be needed to understand this trend.
  

\section{Discussion}
  % summary of results in a big old table.
    % the accuracy of svd and user-user both seemed good. item-item seemed to have trouble especially with very new users.
    % svd did quite well on our traditional topN metrics, item-item did OK, and user-user did bad., but there are flaws with these metrics.
    % on our topN RMSE item-item (again) seemed to have issues, and user-user did quite well, but the low hit rate is troubleing. 
    % again svd seems the winner here.
    % on our fluffy metrics item-item and svd seemed to do well. I might argue that item-item seemed to do better (spread, and higher throughout).
    % user-user seemed to do poor

  % what this means
    % user-user seems to be able to give accurate predictions, but seems to focus on overly obscure items in recommendation.
    % item-item seems to give not-bad recommendations, but its predictions have issues
    % based only on these results svd seems to be the best performer and is our recommendation to new systems.
    
  \begin{table}[ht!]
    \centering
    \begin{tabular}{|p{4.5em}|p{4.5em}|p{7.5em}|p{4.5em}|}
      \hline
      Algorithm & Prediction accuracy & Recommendation quality      & Other properties \\\hline
      ItemItem  & Poor                & Poor                        & Good             \\\hline
      UserUser  & Good                & Inconclusive                & Poor             \\\hline
      SVD       & Good                & Good                        & Good             \\\hline
    \end{tabular}
    \caption{Summary of algorithm quality for new users}
    \label{tbl:results}
  \end{table}

  The high level results of our evaluation are summarized in table \ref{tbl:results}
  Overall we see that SVD seems to be the best performing algorithm for cold start use.
  SVD was the only algorithm that consistently outperformed the baseline algorithms on prediction and recommendation tasks.
  ItemItem had arguably better results on the average popularity and spread metrics, indicating that it was able to find more novel items, and make a larger range of recommendations to users.
  Unfortunately, Item-Item also performed quite poorly at both prediction and recommendation, which may make it less suitable for new users.

  UserUser performed quite well are prediction, but tended to favor unpopular items in its recommendations.
  While some novelty is good, we believe that UserUser goes too far into the obscure movies in its recommendations, and therefore does not make good recommendations.
  Therefore we believe that UserUser is not well suited for new users unless only its predictions are used.
  Exploring corrections for this issue is left as future work.


    % if you use a switching recommender, this methodology can help you choose when to switch.
    % non-traditional metrics can tell you more about what is going on, these seem to be more properties of the class of algo than of the data, so its worth looking at these when choosing algorithms, to find one with properties that are good for your task.
  Interestingly, however, we see that for very small numbers of ratings the UserItemBaseline seems to be our strongest algorithm, both in terms of accuracy (as measured by RMSE), and recommendation quality (as measured by MeanRating@20).
  This suggests a switching recommendation strategy for live systems.
  Instead of recommending from one recommender right away, a system could provide baseline predictions and recommendations until the user has enough ratings and then switching to a better recommender.
  This has the added avantage that it might help users notice the changes in popularity and diversity, helping them appreciate these properties more.
  For switching recommendation strategies like this, our methodology provides a way to find the correct number of ratings at which to switch algorithms.

  % other takeaways
    % regularization
  One possible reason that the SVD algorithm performs as well as it does is the regularization built into the algorithm.
  UserUser and ItemItem algorithms have no built in method for damping their predictions towards the mean when they don't have much data.
  The regularization in the cost function of the SVD model makes it so its predictions will gradually shift away from the baseline predictions as more relevant ratings are added.
  The downside of this, however, could be the higher popularity and lower spread seen by the SVD algorithm.
  The same regularization that helps avoid making mistakes due to too little data, may also prevents the algorithm from taking occasional risks on more obscure movies.
  

  % limitations and opprotunities
    % dataset limitations
      % only one domain
      % no quitters in our dataset

  One of the major limitations of our work is that we only evaluated one dataset.
  Future work should replicate our experiment on other datasets from different recommender domains.
  By doing this we will find out which of our results are true properties of the algorithms, and which results are instead related to properties of our dataset.

  One important property that needs to be considered is that our dataset only contained users that naturally used the system enough to collect 20 ratings.
  Unfortunately, this means that we do not know if there are differences between users who do and do not survive in the system.
  This is a limitation of our methodology, to maintain a constant test set throughout the evaluation we are not able to evaluate users who have less ratings than the maximum amount we wish to study.
  More work needs to be done to understand users who only ever make one or two ratings to understand why they leave, and how they differ from other users of our systems.

  Finally, we feel that much more work should be done using this methodology to explore different algorithms.
  There are many algorithms beyond the three e evaluated.
  Some of these algorithms are designed specifically for cold start.
  Others are designed to leverage extra, non-rating information to improve quality.
  Work should be done to understand the differences between these algorithms, and what effect extra information has on the recommendation process.
  Using our methodology we might be able to come up with an understanding of the value that extra information about the user has on recommendation, and to compare it the value of extra ratings for new users.
    % algorithm limitations
      % only covered three common families,
      % different algos tuned specifically for cold start.
      % algos that use extra data about users or about items.
    % cite the comparison work at CHI.

  % despite the limitations we were able to show differences between algorithms in a cold start situation.


\section{Conclusion}

  Our work shows that a multiple metric approach to understanding recommender behavior can provide useful insights into the differences between algorithms.
  Our methodology presents a list of metrics that can be reused in the future for new multiple metric evaluations, and our methodology shows how this can be applied to understand new users.
  We find that recommending for new users is hard and none of our algorithms can reliably beat a simple baseline for users with very few ratings.
  Once the user has a small number of ratings we find that the SVD algorithm performs best.
  This result can be useful both in designing new recommender systems, and in designing new recommendation algorithms.


\section{Acknowledgements}
  
  The authors would like to express their gratitude towards our colleagues at GroupLens research for their feedback and support while developing this work.
  We would like to specially that Michael Ekstrand for his  important work on the Lenskit recommendation framework.

  This work is supported by NSF grant TODO:

\bibliographystyle{abbrv}
\bibliography{resources}  % sigproc.bib is the name of the Bibliography in this case

\end{document}
